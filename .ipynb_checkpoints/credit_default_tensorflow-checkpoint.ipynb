{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8ff9901d-97f0-cdbc-fda2-bd583869ad71"
   },
   "source": [
    "**Import dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "ad51bff4-3832-d2e2-5649-f8fed94ada4d"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f21bbb073a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "84335cfc-17f9-d23f-9bd9-552684a8672a"
   },
   "source": [
    "**Function to clean the data set and get X and Y. X is hyper parameters, Y is last column in csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "427b25a0-13bd-157d-ec66-65c061054631",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data_location, split_dataset):\n",
    "    dataset = pd.read_csv(data_location)\n",
    "\n",
    "    # 0 shape to get total of rows, 1 to get total of columns\n",
    "    rows = dataset.shape[0]\n",
    "    print (\"there are \", rows, \" rows before cleaning\\n\")\n",
    "\n",
    "    # removing unimportant columns\n",
    "    columns = ['ID']\n",
    "    for text in columns:\n",
    "        del dataset[text]\n",
    "\n",
    "    # get all data except last column\n",
    "    x = dataset.ix[: , :-1].values\n",
    "\n",
    "    # get all data on last column only\n",
    "    y = dataset.ix[: , -1:].values\n",
    "\n",
    "    # split our dataset to reduce overfitting\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = split_dataset)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "93332ce4-33bb-ddfd-56d1-4168204d8e16"
   },
   "source": [
    "**Function to return one-hot-label our Y for softmax cross entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "27e4ac66-5ea3-24a6-4f7c-d2cd4ffb313d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_embedded(x):\n",
    "\n",
    "    data = np.zeros((x.shape[0], np.unique(x).shape[0]), dtype = np.float32)\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        data[i][x[i][0]] = 1.0\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dff3fcbf-8edf-9319-ba32-8a5e069ccfda"
   },
   "source": [
    "**Our global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "38811c3c-4fad-dbda-8c45-ec4876811fe4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_location = '../input/UCI_Credit_Card.csv'\n",
    "\n",
    "# not included input and output layer\n",
    "num_layers = 5\n",
    "# all hidden layers are same wide size\n",
    "size_layer = 64\n",
    "learning_rate = 0.01\n",
    "# batch mini size for training\n",
    "batch_size = 100\n",
    "\n",
    "# beta for regularizer, learn from penalty value\n",
    "beta = 0.05\n",
    "\n",
    "# probability to disconnect connection between nodes\n",
    "prob_dropout = 1.0\n",
    "\n",
    "biased_node = True\n",
    "\n",
    "split_dataset = 0.7\n",
    "\n",
    "# iteration for training\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92b6df36-7dcc-05e7-b1cc-fee4e919428e"
   },
   "source": [
    "**You can choose what type of activation function, I prefer RELu because it does not have upper boundary, and to put penalty later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a2a3e567-7c9d-751f-5a26-64b6aff1fa48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  30000  rows before cleaning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# got sigmoid, softmax, tanh\n",
    "activation = 'relu'\n",
    "\n",
    "if activation == 'sigmoid':\n",
    "    activation = tf.nn.sigmoid\n",
    "elif activation == 'tanh':\n",
    "    activation = tf.nn.tanh\n",
    "elif activation == 'relu':\n",
    "    activation = tf.nn.relu\n",
    "else:\n",
    "    raise Exception(\"model type not supported\")\n",
    "    \n",
    "x_train, x_test, y_train, y_test = get_data(data_location, split_dataset)\n",
    "\n",
    "y_train = return_embedded(y_train)\n",
    "y_test = return_embedded(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ba705cc7-26b9-d089-1241-5edf921cca0e"
   },
   "source": [
    "**Our Deep Dynamic Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "1e0b7720-4928-f66a-79cf-73942e501546",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Network pipelining ===========================================================================\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, x_train.shape[1]])\n",
    "Y = tf.placeholder(\"float\", [None, y_train.shape[1]])\n",
    "        \n",
    "input_layer = tf.Variable(tf.random_normal([x_train.shape[1], size_layer]))\n",
    "\n",
    "if biased_node:\n",
    "    biased_input_layer = tf.Variable(tf.random_normal([size_layer]))\n",
    "    biased = []\n",
    "    for i in range(num_layers):\n",
    "        biased.append(tf.Variable(tf.random_normal([size_layer])))\n",
    "\n",
    "layers = []\n",
    "for i in range(num_layers):\n",
    "    layers.append(tf.Variable(tf.random_normal([size_layer, size_layer])))\n",
    "\n",
    "output_layer = tf.Variable(tf.random_normal([size_layer, y_train.shape[1]]))\n",
    "\n",
    "if biased_node:\n",
    "    first_l = activation(tf.add(tf.matmul(X, input_layer), biased_input_layer))\n",
    "    \n",
    "    # reduce nodes connection\n",
    "    first_l = tf.nn.dropout(first_l, prob_dropout)\n",
    "    \n",
    "    next_l = activation(tf.add(tf.matmul(first_l, layers[0]), biased[0]))\n",
    "    # reduce nodes connection\n",
    "    next_l = tf.nn.dropout(next_l, prob_dropout)\n",
    "    \n",
    "    for i in range(1, num_layers - 1):\n",
    "        next_l = activation(tf.add(tf.matmul(next_l, layers[i]), biased[i]))\n",
    "        \n",
    "        # reduce nodes connection\n",
    "        next_l = tf.nn.dropout(next_l, prob_dropout)\n",
    "else:\n",
    "    first_l = activation(tf.matmul(X, input_layer))\n",
    "    \n",
    "    # reduce nodes connection\n",
    "    first_l = tf.nn.dropout(first_l, prob_dropout)\n",
    "    \n",
    "    next_l = activation(tf.matmul(first_l, layers[0]))\n",
    "    \n",
    "    # reduce nodes connection\n",
    "    next_l = tf.nn.dropout(next_l, prob_dropout)\n",
    "    \n",
    "    for i in range(1, num_layers - 1):\n",
    "        next_l = activation(tf.matmul(next_l, layers[i]))\n",
    "        \n",
    "        # reduce nodes connection\n",
    "        next_l = tf.nn.dropout(next_l, prob_dropout)\n",
    "    \n",
    "last_l = tf.matmul(next_l, output_layer)\n",
    "\n",
    "# adding up all penalties values\n",
    "regularizers = tf.nn.l2_loss(input_layer) + sum(map(lambda x: tf.nn.l2_loss(x), layers)) + tf.nn.l2_loss(output_layer)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = last_l, labels = Y))\n",
    "\n",
    "# included penalty values\n",
    "cost = tf.reduce_mean(cost + beta * regularizers)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(last_l, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "416db8c3-636d-2c40-3782-c6bae62cb92b"
   },
   "source": [
    "**Start our session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "c0349549-b1bc-69b2-81b7-a4021a94ffae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for  100  iteration\n",
      "There are  9000  of rows for training\n"
     ]
    }
   ],
   "source": [
    "# start the session graph\n",
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "# initialize global variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print (\"Train for \", epoch, \" iteration\")\n",
    "print (\"There are \", x_train.shape[0], \" of rows for training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "87177496-63c7-1c39-caa3-d17cb27f3255"
   },
   "source": [
    "**Training session begin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "89c6ba56-4943-d80b-4ec8-4d381dbcb9eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total accuracy:  0.681333331101\n",
      "batch:  1 , loss:  346645.186556 , speed:  0.5706026554107666  s / epoch\n",
      "total accuracy:  0.682555552655\n",
      "batch:  2 , loss:  61689.0918889 , speed:  0.1867964267730713  s / epoch\n",
      "total accuracy:  0.689444441928\n",
      "batch:  3 , loss:  38497.2844514 , speed:  0.20757079124450684  s / epoch\n",
      "total accuracy:  0.691222219335\n",
      "batch:  4 , loss:  22804.1198333 , speed:  0.24881815910339355  s / epoch\n",
      "total accuracy:  0.695666663514\n",
      "batch:  5 , loss:  20792.4955833 , speed:  0.20152068138122559  s / epoch\n",
      "total accuracy:  0.692777779698\n",
      "batch:  6 , loss:  19658.3189792 , speed:  0.20322895050048828  s / epoch\n",
      "total accuracy:  0.69044444561\n",
      "batch:  7 , loss:  13691.4251944 , speed:  0.220780611038208  s / epoch\n",
      "total accuracy:  0.695777778824\n",
      "batch:  8 , loss:  10199.5605799 , speed:  0.2102794647216797  s / epoch\n",
      "total accuracy:  0.691999999682\n",
      "batch:  9 , loss:  11461.6478681 , speed:  0.20798277854919434  s / epoch\n",
      "total accuracy:  0.692444444696\n",
      "batch:  10 , loss:  8363.30952083 , speed:  0.22700190544128418  s / epoch\n",
      "total accuracy:  0.688777778546\n",
      "batch:  11 , loss:  6855.02603125 , speed:  0.19800043106079102  s / epoch\n",
      "total accuracy:  0.685444448392\n",
      "batch:  12 , loss:  5888.46881424 , speed:  0.20998716354370117  s / epoch\n",
      "total accuracy:  0.690555556284\n",
      "batch:  13 , loss:  4770.36303472 , speed:  0.18879127502441406  s / epoch\n",
      "total accuracy:  0.696999998556\n",
      "batch:  14 , loss:  3840.03711545 , speed:  0.2005634307861328  s / epoch\n",
      "total accuracy:  0.690777778625\n",
      "batch:  15 , loss:  5062.08491319 , speed:  0.20492911338806152  s / epoch\n",
      "total accuracy:  0.689222225216\n",
      "batch:  16 , loss:  3788.07811285 , speed:  0.21015119552612305  s / epoch\n",
      "total accuracy:  0.689555555913\n",
      "batch:  17 , loss:  2961.70984983 , speed:  0.22042012214660645  s / epoch\n",
      "total accuracy:  0.691111113628\n",
      "batch:  18 , loss:  2769.74130035 , speed:  0.2360684871673584  s / epoch\n",
      "total accuracy:  0.699222222302\n",
      "batch:  19 , loss:  3710.33782813 , speed:  0.2283775806427002  s / epoch\n",
      "total accuracy:  0.576777774758\n",
      "batch:  20 , loss:  3639.32340495 , speed:  0.2808401584625244  s / epoch\n",
      "total accuracy:  0.712888887856\n",
      "batch:  21 , loss:  1049.24019665 , speed:  0.3016853332519531  s / epoch\n",
      "total accuracy:  0.755777778725\n",
      "batch:  22 , loss:  546.896754815 , speed:  0.31720781326293945  s / epoch\n",
      "total accuracy:  0.771444445848\n",
      "batch:  23 , loss:  443.604800269 , speed:  0.29726552963256836  s / epoch\n",
      "total accuracy:  0.762666668163\n",
      "batch:  24 , loss:  368.444429525 , speed:  0.2793097496032715  s / epoch\n",
      "total accuracy:  0.775444444021\n",
      "batch:  25 , loss:  97.9895295817 , speed:  0.28826165199279785  s / epoch\n",
      "total accuracy:  0.774111111297\n",
      "batch:  26 , loss:  93.1448597446 , speed:  0.28533029556274414  s / epoch\n",
      "total accuracy:  0.775111114317\n",
      "batch:  27 , loss:  62.3647577006 , speed:  0.29963088035583496  s / epoch\n",
      "total accuracy:  0.776222223043\n",
      "batch:  28 , loss:  31.0393979967 , speed:  0.30633974075317383  s / epoch\n",
      "total accuracy:  0.777777780427\n",
      "batch:  29 , loss:  31.0438241204 , speed:  0.2962684631347656  s / epoch\n",
      "total accuracy:  0.778111112118\n",
      "batch:  30 , loss:  62.7009169617 , speed:  0.28809142112731934  s / epoch\n",
      "total accuracy:  0.777333334419\n",
      "batch:  31 , loss:  31.8699912381 , speed:  0.2966625690460205  s / epoch\n",
      "total accuracy:  0.777222223414\n",
      "batch:  32 , loss:  27.8310259026 , speed:  0.28571581840515137  s / epoch\n",
      "total accuracy:  0.778444445133\n",
      "batch:  33 , loss:  48.2418713684 , speed:  0.3134734630584717  s / epoch\n",
      "total accuracy:  0.778222223123\n",
      "batch:  34 , loss:  31.5805443726 , speed:  0.2954750061035156  s / epoch\n",
      "total accuracy:  0.778888889154\n",
      "batch:  35 , loss:  18.3723982951 , speed:  0.29412841796875  s / epoch\n",
      "total accuracy:  0.779111111826\n",
      "batch:  36 , loss:  16.5736098904 , speed:  0.3037424087524414  s / epoch\n",
      "total accuracy:  0.779111111826\n",
      "batch:  37 , loss:  13.0368181491 , speed:  0.3076608180999756  s / epoch\n",
      "total accuracy:  0.779333333174\n",
      "batch:  38 , loss:  13.0986454942 , speed:  0.29103922843933105  s / epoch\n",
      "total accuracy:  0.778444445133\n",
      "batch:  39 , loss:  12.2121425612 , speed:  0.3044447898864746  s / epoch\n",
      "total accuracy:  0.778444445795\n",
      "batch:  40 , loss:  31.3791187337 , speed:  0.34250807762145996  s / epoch\n",
      "total accuracy:  0.745444444815\n",
      "batch:  41 , loss:  53.9662370063 , speed:  0.33193278312683105  s / epoch\n",
      "total accuracy:  0.750777779023\n",
      "batch:  42 , loss:  64.2428790521 , speed:  0.29947757720947266  s / epoch\n",
      "total accuracy:  0.77322222458\n",
      "batch:  43 , loss:  29.3530265605 , speed:  0.3113534450531006  s / epoch\n",
      "total accuracy:  0.763333333366\n",
      "batch:  44 , loss:  525.824084819 , speed:  0.38155317306518555  s / epoch\n",
      "total accuracy:  0.751888890068\n",
      "batch:  45 , loss:  297.217009884 , speed:  0.33947062492370605  s / epoch\n",
      "total accuracy:  0.733222223653\n",
      "batch:  46 , loss:  147.151343791 , speed:  0.33831071853637695  s / epoch\n",
      "total accuracy:  0.778111111455\n",
      "batch:  47 , loss:  107.545846503 , speed:  0.3170197010040283  s / epoch\n",
      "total accuracy:  0.778555556801\n",
      "batch:  48 , loss:  37.4593801982 , speed:  0.3078632354736328  s / epoch\n",
      "total accuracy:  0.778333335453\n",
      "batch:  49 , loss:  6.77261891005 , speed:  0.304271936416626  s / epoch\n",
      "total accuracy:  0.77833333479\n",
      "batch:  50 , loss:  5.6204258355 , speed:  0.30773305892944336  s / epoch\n",
      "total accuracy:  0.778777779473\n",
      "batch:  51 , loss:  4.66188413832 , speed:  0.3202483654022217  s / epoch\n",
      "total accuracy:  0.778777779473\n",
      "batch:  52 , loss:  4.17170715332 , speed:  0.3159816265106201  s / epoch\n",
      "total accuracy:  0.778777779473\n",
      "batch:  53 , loss:  3.81286018202 , speed:  0.3101508617401123  s / epoch\n",
      "total accuracy:  0.778444445795\n",
      "batch:  54 , loss:  4.45917837863 , speed:  0.32032275199890137  s / epoch\n",
      "total accuracy:  0.778777779473\n",
      "batch:  55 , loss:  3.84151232232 , speed:  0.3322603702545166  s / epoch\n",
      "total accuracy:  0.778666667806\n",
      "batch:  56 , loss:  4.02870221625 , speed:  0.32538437843322754  s / epoch\n",
      "total accuracy:  0.778666668468\n",
      "batch:  57 , loss:  5.33165912204 , speed:  0.3330655097961426  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  58 , loss:  3.70440122816 , speed:  0.3063473701477051  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  59 , loss:  3.688574273 , speed:  0.31916236877441406  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  60 , loss:  3.68538259549 , speed:  0.3247489929199219  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  61 , loss:  3.68219342719 , speed:  0.359591007232666  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  62 , loss:  3.67900775825 , speed:  0.31372928619384766  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  63 , loss:  3.67582571411 , speed:  0.31633687019348145  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  64 , loss:  3.67264868503 , speed:  0.3318653106689453  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  65 , loss:  3.66947760688 , speed:  0.3461027145385742  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  66 , loss:  3.66631402249 , speed:  0.3451356887817383  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  67 , loss:  3.66315906779 , speed:  0.33541202545166016  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  68 , loss:  3.66177274916 , speed:  0.34729480743408203  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  69 , loss:  3.65807456462 , speed:  0.32459211349487305  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  70 , loss:  3.654898251 , speed:  0.33820629119873047  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  71 , loss:  3.6517567749 , speed:  0.3072042465209961  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  72 , loss:  3.64863615587 , speed:  0.2929198741912842  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  73 , loss:  3.64553272502 , speed:  0.33546972274780273  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  74 , loss:  3.64244547865 , speed:  0.31164979934692383  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  75 , loss:  3.63937245009 , speed:  0.33229732513427734  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  76 , loss:  3.6363129069 , speed:  0.27670812606811523  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  77 , loss:  3.63326598443 , speed:  0.2993309497833252  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  78 , loss:  3.63022914632 , speed:  0.29365110397338867  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  79 , loss:  3.62720180257 , speed:  0.2905848026275635  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  80 , loss:  3.62418159654 , speed:  0.3413562774658203  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  81 , loss:  3.62116611735 , speed:  0.3019535541534424  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  82 , loss:  3.61815300836 , speed:  0.2946467399597168  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  83 , loss:  3.61513940091 , speed:  0.3132774829864502  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  84 , loss:  3.61212132772 , speed:  0.300403356552124  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  85 , loss:  3.60909675429 , speed:  0.3096787929534912  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  86 , loss:  3.60606213718 , speed:  0.3138749599456787  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  87 , loss:  3.60301335992 , speed:  0.3579983711242676  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  88 , loss:  3.59994727241 , speed:  0.33159518241882324  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  89 , loss:  3.59685979546 , speed:  0.3147928714752197  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  90 , loss:  3.59374722968 , speed:  0.33841538429260254  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  91 , loss:  3.59060489231 , speed:  0.3148965835571289  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  92 , loss:  3.58743009101 , speed:  0.3177201747894287  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  93 , loss:  3.58421851942 , speed:  0.38698434829711914  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  94 , loss:  3.58096605428 , speed:  0.32555484771728516  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  95 , loss:  3.5776684672 , speed:  0.3093447685241699  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  96 , loss:  3.57432237413 , speed:  0.3232228755950928  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  97 , loss:  3.57092383152 , speed:  0.29836010932922363  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  98 , loss:  3.56746898058 , speed:  0.30625200271606445  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  99 , loss:  3.56395420668 , speed:  0.3188602924346924  s / epoch\n",
      "total accuracy:  0.778888890478\n",
      "batch:  100 , loss:  3.56037585788 , speed:  0.31768321990966797  s / epoch\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    last_time = time.time()\n",
    "    total_lost = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    for n in range(0, x_train.shape[0], batch_size):\n",
    "        out, _, loss = sess.run([accuracy, optimizer, cost], feed_dict={X: x_train[n : n + batch_size, :], Y: y_train[n : n + batch_size, :]})\n",
    "        total_accuracy += out\n",
    "        total_lost += loss\n",
    "    \n",
    "    print (\"total accuracy: \", total_accuracy / (x_train.shape[0] / batch_size * 1.0))\n",
    "    diff = time.time() - last_time\n",
    "    print (\"batch: \", i + 1, \", loss: \", total_lost/x_train.shape[0], \", speed: \", diff, \" s / epoch\")\n",
    "    total_lost = 0\n",
    "    total_accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ff2a35c6-1769-8b80-ae84-a3b8943e0d8b"
   },
   "source": [
    "Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "e01fe1c7-d463-76b9-01d1-edf7eb2684d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct positive:  7  /  4638\n",
      "total correct:  16356  /  21000\n",
      "total accuracy:  0.778857142857\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_positive = 0\n",
    "total_correct_positive = 0\n",
    "for n in range(x_test.shape[0]):\n",
    "    \n",
    "    correct = sess.run(accuracy, feed_dict={X: x_test[n : n + 1, :], Y: y_test[n : n + 1 , :]})\n",
    "    total_correct += correct\n",
    "    if y_test[n][1] == 1:\n",
    "        total_positive += 1\n",
    "        if correct == 1:\n",
    "            total_correct_positive += 1\n",
    "    \n",
    "print (\"total correct positive: \", total_correct_positive, \" / \", total_positive)\n",
    "print (\"total correct: \", int(total_correct), \" / \", x_test.shape[0]) \n",
    "print (\"total accuracy: \", total_correct / (x_test.shape[0] * 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c2899b8-1966-36e8-d6c5-f8f866f02b19"
   },
   "source": [
    "**The output generated overly biased/under fitted. You need to increase the iteration atleast one thousand if want better result.**"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 3,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
